---
{"dg-publish":true,"permalink":"/02 resources/西瓜书3 线性模型/","created":"2025-03-12T22:16:39.240+08:00","updated":"2025-03-16T15:14:24.540+08:00"}
---

## 3.1 基本形式
给定由d个属性描述的示例，其中 $x_i$ 是x在第i个属性上的取值，线性模型就是试图学得一个通过属性的线性组合来进行预测的函数即：

$$f(x) = w_1x_1 + w_2x_2 + \ldots + w_dx_d + b$$
向量形式：
$$f(x) = w^Tx + b$$
$$w = (w_1; w_2; \ldots; w_d)$$

w和b学得之后，模型即可确定。其中w还可以表示为属性的权重，下面是一个例子：
- 综合考虑色泽、根蒂和敲声来判断西瓜好不好
- 其中根蒂的系数最大，表明根蒂最要紧；而敲声的系数比色泽大，说明敲声比色泽更重要

$$f_{\text{好瓜}}(x) = 0.2 \cdot x_{\text{色泽}} + 0.5 \cdot x_{\text{根蒂}} + 0.3 \cdot x_{\text{敲声}} + 1$$
## **3.2 线性回归**
线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，
### 一元线性回归
线性回归试图学得一个线性函数，使得对于每个样本 $i$，预测值 $f(x_i)$ 尽可能接近真实值 $y_i$。

$$f(x_i) = wx_i + b$$
通过最小化[[03 pages/301机器学习/损失函数\|损失函数]] $E(w, b)$，我们可以得到最优的权重向量 $w$ 和偏置项 $b$，从而得到最佳的线性回归模型。
损失函数定义为（==均方误差==）：
$$E(w, b) = \sum_{i=1}^{m} (y_i - wx_i - b)^2$$
其中，$m$ 是样本数量，$y_i$ 是第 $i$ 个样本的真实值，$x_i$ 是第 $i$ 个样本的特征向量。
均方误差有非常好的几何意义，它对应了常用的**欧几里得距离**或简称“欧氏距离”(Euclidean distance). 基于均方误差最小化来进行模型求解的方法称为“[[最小二乘法\|最小二乘法]]”(least square method). 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。
==求解过程==
最优的 $w$ 和 $b$ 可以表示为：
$$(w^*, b^*) = \arg \min_{(w, b)} \sum_{i=1}^{m} (f(x_i) - y_i)^2$$
其中，$f(x_i) = wx_i + b$。
这等价于最小化以下表达式：
$$(w^*, b^*) = \arg \min_{(w, b)} \sum_{i=1}^{m} (y_i - wx_i - b)^2$$

为了最小化损失函数 $E(w, b)$，我们需要分别对 $w$ 和 $b$ ==求偏导数，并令其等于零==。

> [!NOTE] 凸函数⭐
>  这里E (w, b) 是关于w和b的凸函数，当它关于w和b的导数均为零时，得到w和b的最优解。对实数集上的函数，可通过求二阶导数来判别：若二阶导数在区间上非负，则称为凸函数；若二阶导数在区间上恒大于0, 则称为严格凸函数。

对 $w$ 求偏导数：
$$\frac{\partial E(w, b)}{\partial w} = 2 \left( w \sum_{i=1}^{m} x_i^2 - \sum_{i=1}^{m} (y_i - b) x_i \right)$$
对 $b$ 求偏导数：
$$\frac{\partial E(w, b)}{\partial b} = 2 \left( mb - \sum_{i=1}^{m} (y_i - wx_i) \right)$$
令上述两个偏导数等于零，我们可以得到两个方程，解这个方程组就可以得到最优的 $w$ 和 $b$。

> [!ABSTRACT] 符号"$\arg \min$"
> 其中"arg"是"argument"（参数）的前三个字母，"min" 是"minimum"（最小值）的前三个字母，该符号表示求使目标函数达到最小值的参数取值。 
> "$\min$"和"$\arg \min$"的区别在于，前者输出目标函数的最小值，而后者输出使得目标函数达到最小值时的参数取值。

> [!TIP] 属性数值化💡
>  为了能进行数学运算，样本中的非数值类属性都需要进行数值化。对于存在"序"关系的属性，可通过连续化将其转化为带有相对大小关系的连续值；对于不存在"序"关系的属性，可根据属性取值将其拆解为多个属性 (one-hot 编码)

> [!NOTE] 闭式解⭐
> "闭式解"或称为"解析解"。闭式解是指可以通过具体的表达式解出待解参数，机器学习算法很少有闭式解，线性回归是一个特例 

### 多元线性回归
更一般的情形是样本由 $d$ 个属性描述，此时我们试图学得：
$$
f(x_i) = w^T x_i + b \quad \text{使得} \quad f(x_i) \simeq y_i
$$
对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：
将参数向量 $\hat{w}$ 表示为：
$$
\hat{w} = (\omega; b) = \begin{pmatrix} \omega_1 \\ \omega_2 \\ \vdots \\ \omega_d \\ b \end{pmatrix} \quad (d+1) \text{维}
$$
输入矩阵 $X$ 表示为：
$$
X = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1d} & 1 \\ x_{21} & x_{22} & \cdots & x_{2d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ x_{m1} & x_{m2} & \cdots & x_{md} & 1 \end{pmatrix} = \begin{pmatrix} a_1^T & 1 \\ a_2^T & 1 \\ \vdots & \vdots \\ a_m^T & 1 \end{pmatrix} \quad m \times (d+1)
$$
将输入矩阵 $X$ 与参数向量 $\hat{w}$ 相乘，得到预测值：
$$
X * \hat{w} = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1d} & 1 \\ x_{21} & x_{22} & \cdots & x_{2d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ x_{m1} & x_{m2} & \cdots & x_{md} & 1 \end{pmatrix} * \begin{pmatrix} \omega_1 \\ \omega_2 \\ \vdots \\ \omega_d \\ b \end{pmatrix} = \begin{pmatrix} \omega_1 x_{11} + \omega_2 x_{12} + \cdots + \omega_d x_{1d} + b \\ \omega_1 x_{21} + \omega_2 x_{22} + \cdots + \omega_d x_{2d} + b \\ \vdots \\ \omega_1 x_{m1} + \omega_2 x_{m2} + \cdots + \omega_d x_{md} + b \end{pmatrix} = \begin{pmatrix} f(x_1) \\ f(x_2) \\ \vdots \\ f(x_m) \end{pmatrix}
$$
最小二乘法的目标是最小化预测值与实际值之间的误差平方和：
$$
\hat{w}^* = \arg \min_{\hat{w}} \left( y - X\hat{w} \right)^T \left( y - X\hat{w} \right)
$$
> [!ABSTRACT] 解释📔
>  **$\arg \min_{\hat{w}}$*
>  - **含义**：表示“使得后面的表达式最小化的 $\hat{w}$ 值”。
>  - **作用**：这是一个数学符号，用于表示我们在所有可能的 $\hat{w}$ 值中寻找一个特定的值，这个值使得后面的误差函数最小。
>  
>  **$\left( y - X\hat{w} \right)^T$**
>  - **含义**：表示预测值与实际值之差的转置。
>  - **作用**：$y$ 是实际值向量，$X\hat{w}$ 是预测值向量。它们的差 $y - X\hat{w}$ 表示每个样本的预测误差。转置操作 $^T$ 是将向量或矩阵的行和列互换，为后续的矩阵乘法做准备。
>  
>  **$\left( y - X\hat{w} \right)$*
>  - **含义**：表示预测值与实际值之差。
>  - **作用**：与上面的转置部分相乘，计算误差的平方和。

同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0。
> [!NOTE] tip⭐
>  当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置\*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算 (比如[[03 pages/301机器学习/正则化\|正则化]])

对 $\hat{w}$ 求导，得到：
$$
\frac{\partial E_{\hat{w}}}{\partial \hat{w}} = 2X^T \left( X\hat{w} - y \right) = 0
$$
解上述方程，得到最优解的闭式解：
$$
\hat{w}^* = \left( X^TX \right)^{-1} X^T y
$$
其中，$X^TX$ 需要是满秩矩阵或非奇异矩阵，以确保逆矩阵存在。
使用最优解 $\hat{w}^*$ 进行预测：
$$
f(x_i) = \hat{x}_i^T \left( X^TX \right)^{-1} X^T y
$$


### 对数线性回归
假设我们认为示例所对应的输出标记实在指数尺度上变化，那么可将输出标记的对数作为线性模型逼近的目标，也就是：
$$\ln y = w^\mathrm{T}x + b$$
这就是对数线性回归，他实际上是试图让e的指数次方趋近于y，表示图如下：
![Pasted image 20250315213525.png](/img/user/09%20settings/Z%20attachment/Pasted%20image%2020250315213525.png)

更一般地，考虑所有y的衍生物的情形，就得到了“==广义的线性模型==”（generalized linear model），其中，g（\*）称为==联系函数==（link function）。
$$y = g^{-1}(w^\mathrm{T}x + b)$$
对数线性回归是当 $g(\cdot) = \ln(\cdot)$ 时广义线性模型的特例。

## **3.2 线性几率回归**
如何将线性模型用于分类任务？
只需找一个单调函数将分类任务的真实标记与预测值联系起来。二分类中，我们可以利用阶跃函数进行理想分类：
$$y = \begin{cases} 
0, & z < 0; \\
0.5, & z = 0; \\
1, & z > 0 
\end{cases}$$
但是有个缺陷就是阶跃函数不连续，由此我们找到一个符合上述条件的替代函数, 该函数也叫对数几率函数 (logistic functin) 这是一种“Sigmoid函数”（Sigmoid函数即形似S的函数）它将z值转化为一个接近0或1的y值，并且其输出值在z = 0附近变化很陡。
$$y = \frac{1}{1 + e^{-z}}$$
![Pasted image 20250315214236.png](/img/user/09%20settings/Z%20attachment/Pasted%20image%2020250315214236.png)
将对数几率函数作为g (•) 代入 $y = g^{-1}(w^\mathrm{T}x + b)$，得到 $y = \frac{1}{1 + e^{-(w^\mathrm{T}x + b)}}$ 可变化为
$$\ln \frac{y}{1-y} = w^\mathrm{T}x + b$$
若将 y 视为样本 $\pmb{x}$ 作为正例的可能性，则 1-y 是其反例可能性，两者的比值
$$\frac{y}{1-y}$$
称为”==几率==“（odds），反映了 $\pmb{x}$ 作为正例的相对可能性。对几率取对数则得到”==对数几率=="（log odds，亦称logit）

$$\ln\frac{y}{1-y}$$
因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“[[03 pages/301机器学习/逻辑回归\|逻辑回归]]”。虽然它的名字是“回归”，但实际却是一种**分类学习方法**。
这种方法有很多优点例如它是直接对分类可能性进行建模,无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题；它不是仅预测出“类别”，而是可得到**近似概率预测**，这对许多需利用概率辅助决策的任务很有用；此外，对率函数是任意阶可导的**凸函数**，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。
我们可通过极大似然法” (maximum likelihood method) 来估计ωΤ和b。（详细推导  #待补充

## **3.3 线性判别分析**
线性判别分析（Linear Discriminant Analysis，简称LDA）,在二分类问题上因为最早由［Fisher，1936］提出，亦称"Fisher判别分析”。【严格说来LDA与Fisher判别分析稍有不同，前者假设了各类样本的协方差矩阵相同且满秩。】
其基本思想是：<u>将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远</u>。如图所示：
![13.png](https://i.loli.net/2018/10/17/5bc723b863ebb.png)

线性判别分析也是一种降维方法，但不同于第10章介绍的无监督降维方法，线性判别分析是一种监督降维方法，即降维过程中需要用到样本类别标记信息。

给定数据集$D=\{(\pmb{x}_i,y_i)\}_{i=1}^m,y_i\in\{0,1\}$，令$X_i$、$\pmb{\mu}_i$、$\mathbf{\Sigma}_i$分别表示第$i\in\{0,1\}$类示例的集合、均值向量、[[00 inbox/协方差矩阵\|协方差矩阵]]。若将数据投影到直线$\pmb{w}$上，则两类样本的中心在直线上的投影分别为$\pmb{w}^\top\pmb{\mu}_0$和$\pmb{w}^\top\pmb{\mu}_1$；若将所有样本点都投影到直线上，则两类样本的协方差分别为$\pmb{w}^\top\mathbf{\Sigma}_0\pmb{w}$和$\pmb{w}^\top\mathbf{\Sigma}_1\pmb{w}$。由于直线是一维空间，因此$\pmb{w}^\top\pmb{\mu}_0$、$\pmb{w}^\top\pmb{\mu}_1$、$\pmb{w}^\top\mathbf{\Sigma}_0\pmb{w}$、$\pmb{w}^\top\mathbf{\Sigma}_1\pmb{w}$均为实数。

欲使同样样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即 $\pmb{w}^\top\mathbf{\Sigma}_0\pmb{w}+\pmb{w}^\top\mathbf{\Sigma}_1\pmb{w}$ 尽可能小；而欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即 $\|\pmb{w}^\top\pmb{\mu}_0-\pmb{w}^\top\pmb{\mu}_1\|_2^2$ 尽可能大。同时考虑二者，则可得到欲最大化的目标
$$
\begin{align}
J&=\frac{\|\pmb{w}^\top\pmb{\mu}_0-\pmb{w}^\top\pmb{\mu}_1\|_2^2}{\pmb{w}^\top\mathbf{\Sigma}_0\pmb{w}+\pmb{w}^\top\mathbf{\Sigma}_1\pmb{w}} \\
&=\frac{\|(\pmb{w}^\top\pmb{\mu}_0-\pmb{w}^\top\pmb{\mu}_1)^\top\|_2^2}{\pmb{w}^\top\mathbf{\Sigma}_0\pmb{w}+\pmb{w}^\top\mathbf{\Sigma}_1\pmb{w}} \\
&=\frac{\|(\pmb{\mu}_0-\pmb{\mu}_1)^\top\pmb{w}\|_2^2}{\pmb{w}^\top\mathbf{\Sigma}_0\pmb{w}+\pmb{w}^\top\mathbf{\Sigma}_1\pmb{w}} \\
&=\frac{\left[(\pmb{\mu}_0-\pmb{\mu}_1)^\top\pmb{w}\right]^\top(\pmb{\mu}_0-\pmb{\mu}_1)^\top\pmb{w}}{\pmb{w}^\top(\mathbf{\Sigma}_0+\mathbf{\Sigma}_1)\pmb{w}} \\
&=\frac{\pmb{w}^\top(\pmb{\mu}_0-\pmb{\mu}_1)(\pmb{\mu}_0-\pmb{\mu}_1)^\top\pmb{w}} {\pmb{w}^\top(\mathbf{\Sigma}_0+\mathbf{\Sigma}_1)\pmb{w}} \tag{3.32}
\end{align}
$$

定义“类内散度矩阵”（within-class scatter matrix）【同一类的样本之间的“紧密程度”。】
$$
\begin{align}
\mathbf{S}_w&=\mathbf{\Sigma}_0+\mathbf{\Sigma}_1 \\
&=\sum_{\pmb{x}\in X_0}(\pmb{x}-\pmb{\mu}_0)(\pmb{x}-\pmb{\mu}_0)^\top+ \sum_{\pmb{x}\in X_1}(\pmb{x}-\pmb{\mu}_1)(\pmb{x}-\pmb{\mu}_1)^\top
\end{align}
$$

以及”类间散度矩阵“（between-class scatter matrix)【不同类别之间的“分开程度”。】
$$\mathbf{S}_b=(\pmb{\mu}_0-\pmb{\mu}_1)(\pmb{\mu}_0-\pmb{\mu}_1)^\top$$
则式（3.32）可重写为
$$J=\frac{\pmb{w}^\top\mathbf{S}_b\pmb{w}}{\pmb{w}^\top\mathbf{S}_w\pmb{w}} \tag{3.35}$$
这就是LDA欲最大化的目标，即 $\mathbf{S}_b$ 与 $\mathbf{S}_w$ 的”广义瑞利商”（generalized Rayleigh quotient）。（详细推导 #待补充

## **3.4 多分类学习**

现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。
最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。

- OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。
- OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。
- MvM：每次将若干个类作为正例，若干个其他类作为反例，OvO和OvR是MvM的特例。正反类的构造有特殊的设计，一种常用的方法：纠错输出码(ECOC)，将编码的思想引入类别拆分，尽可能在解码过程中具有容错性，主要分为两步：
	- 编码：对N个类做M次划分，每次划分将一部分类作为正类，一部分类作为反类，形成一个二分类训练集；一共产生M个训练集，可训练出M个分类器。  
	- 解码：M个分类器分别对测试样本进行预测，这些预测标记组成一个编码，将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。
    

![Pasted image 20250316144156.png](/img/user/09%20settings/Z%20attachment/Pasted%20image%2020250316144156.png)
![Pasted image 20250316145004.png](/img/user/09%20settings/Z%20attachment/Pasted%20image%2020250316145004.png)


![Pasted image 20250316144233.png](/img/user/09%20settings/Z%20attachment/Pasted%20image%2020250316144233.png)
注："==海明距离=="是指两个码对应位置不相同的个数，"==欧式距离=="则是指两个向量之间的欧氏距离

> [!TIP] 通俗理解💡
>  纠错输出码(Error-Correcting Output Codes)是把N个类别改写成M个二分类问题的密码系统。就像把汉字转成电报码，每个数字代表某个特征问题的答案。数学表达式就是：类别→密码本→分类器阵列→解码器→预测
>  
>  （翻开密码本指着说）假设我们用5个问题来定义宠物：
>  
>  1. 会不会游泳？
>  2. 有没有羽毛？
>  3. 是不是哺乳动物？
>  4. 能不能飞？
>  5. 吃不吃鱼？
>  
>  这时：
>  
>  - 鱼的密码可能是[-1, -1, -1, -1, +1]（不会飞但吃鱼）
>  - 鸟的密码是[-1, +1, -1, +1, -1]
>  （突然举起个毛绒玩具）现在有只未知生物，5个分类器给出[-1, +1, -1, +1, -1]，和鸟的密码完全匹配！但如果有两个答案出错变成[-1, -1, -1, +1, -1]，系统会发现它和鱼的差距是2，和鸟的差距是1，仍然能正确识别。
> - 也就是说，每个分类器分两堆，有很多分类器，把这些分类结果用编码记录下来。这样形成了一个预测分类的向量，与真实的分类向量比较，看哪个更近。
## **3.5 类别不平衡问题**
类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：
1. 在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。
2. 在训练样本较少的类别中进行“过采样”（oversampling）, 例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。过采样不能简单地对初始正例样本进行重复采样，否则会招致严重的过拟合。
3. 直接基于原数据集进行学习，对预测值进行“再缩放”处理。称为“阈值移动” (threshold-moving)。其中再缩放也是代价敏感学习的基础。
    ![Pasted image 20250316145131.png](/img/user/09%20settings/Z%20attachment/Pasted%20image%2020250316145131.png)

> [!TIP] 形象化理解
>  - **欠采样**：让多数派裁员（抓出100个精英正例）
>      - EasyEnsemble秘术：把900正例分成9个100人议会，每个议会和100反例比武，最后九大门派投票
>  - **过采样**：给少数派克隆大军（用SMOTE魔法造新反例）
>      - SMOTE炼金术：在现有反例间架设彩虹桥，在特征空间里变出新样本，就像用两杯咖啡勾兑出第三杯
>  - **阈值移动**：修改判决门槛（原被告适用不同法律）
>      - 再缩放诡计：把分类阈值从0.5改成(反例数量/正例数量)，好比法官说"原告证据需达到被告10倍强度才胜诉"
>  
>  **特征擂台**
>  
> |流派|必杀技|破绽|
> |---|---|---|
> |裁员流|运算快如闪电|可能裁掉关键情报|
> |克隆流|保留所有线索|容易制造替身攻击|
> |改规流|不碰原始数据|需要精确概率估测|
>  
>  （突然展开张医院报告）假设我们要检测罕见病：
> 
>  - 原始数据：健康人900例，患者100例
>  - 欠采样：随机选100健康人+100患者，但可能漏掉特殊病例
>  - 过采样：用患者数据生成900新病例，可能造出"长三只手"的虚假病人
>  - 阈值移动：告诉模型"只要30%概率就判定患病"，但需确保概率计算准确


