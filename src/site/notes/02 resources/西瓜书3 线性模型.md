---
{"dg-publish":true,"permalink":"/02 resources/西瓜书3 线性模型/","created":"2025-03-12T22:16:39.240+08:00","updated":"2025-03-14T22:52:04.199+08:00"}
---

## 3.1 基本形式
给定由d个属性描述的示例，其中 $x_i$ 是x在第i个属性上的取值，线性模型就是试图学得一个通过属性的线性组合来进行预测的函数即：

$$f(x) = w_1x_1 + w_2x_2 + \ldots + w_dx_d + b$$
向量形式：
$$f(x) = w^Tx + b$$
$$w = (w_1; w_2; \ldots; w_d)$$

w和b学得之后，模型即可确定。其中w还可以表示为属性的权重，下面是一个例子：
- 综合考虑色泽、根蒂和敲声来判断西瓜好不好
- 其中根蒂的系数最大，表明根蒂最要紧；而敲声的系数比色泽大，说明敲声比色泽更重要

$$f_{\text{好瓜}}(x) = 0.2 \cdot x_{\text{色泽}} + 0.5 \cdot x_{\text{根蒂}} + 0.3 \cdot x_{\text{敲声}} + 1$$
## **3.1 线性回归**
线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，
### 一元线性回归
线性回归试图学得一个线性函数，使得对于每个样本 $i$，预测值 $f(x_i)$ 尽可能接近真实值 $y_i$。

$$f(x_i) = wx_i + b$$
通过最小化[[03 pages/301机器学习/损失函数\|损失函数]] $E(w, b)$，我们可以得到最优的权重向量 $w$ 和偏置项 $b$，从而得到最佳的线性回归模型。
损失函数定义为（==均方误差==）：
$$E(w, b) = \sum_{i=1}^{m} (y_i - wx_i - b)^2$$
其中，$m$ 是样本数量，$y_i$ 是第 $i$ 个样本的真实值，$x_i$ 是第 $i$ 个样本的特征向量。
均方误差有非常好的几何意义，它对应了常用的**欧几里得距离**或简称“欧氏距离”(Euclidean distance). 基于均方误差最小化来进行模型求解的方法称为“[[最小二乘法\|最小二乘法]]”(least square method). 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。
==求解过程==
最优的 $w$ 和 $b$ 可以表示为：
$$(w^*, b^*) = \arg \min_{(w, b)} \sum_{i=1}^{m} (f(x_i) - y_i)^2$$
其中，$f(x_i) = wx_i + b$。
这等价于最小化以下表达式：
$$(w^*, b^*) = \arg \min_{(w, b)} \sum_{i=1}^{m} (y_i - wx_i - b)^2$$

为了最小化损失函数 $E(w, b)$，我们需要分别对 $w$ 和 $b$ ==求偏导数，并令其等于零==。

> [!NOTE] 凸函数⭐
>  这里E (w, b) 是关于w和b的凸函数，当它关于w和b的导数均为零时，得到w和b的最优解。对实数集上的函数，可通过求二阶导数来判别：若二阶导数在区间上非负，则称为凸函数；若二阶导数在区间上恒大于0, 则称为严格凸函数。

对 $w$ 求偏导数：
$$\frac{\partial E(w, b)}{\partial w} = 2 \left( w \sum_{i=1}^{m} x_i^2 - \sum_{i=1}^{m} (y_i - b) x_i \right)$$
对 $b$ 求偏导数：
$$\frac{\partial E(w, b)}{\partial b} = 2 \left( mb - \sum_{i=1}^{m} (y_i - wx_i) \right)$$
令上述两个偏导数等于零，我们可以得到两个方程，解这个方程组就可以得到最优的 $w$ 和 $b$。

> [!ABSTRACT] 符号"$\arg \min$"
> 其中"arg"是"argument"（参数）的前三个字母，"min" 是"minimum"（最小值）的前三个字母，该符号表示求使目标函数达到最小值的参数取值。 
> "$\min$"和"$\arg \min$"的区别在于，前者输出目标函数的最小值，而后者输出使得目标函数达到最小值时的参数取值。

> [!TIP] 属性数值化💡
>  为了能进行数学运算，样本中的非数值类属性都需要进行数值化。对于存在"序"关系的属性，可通过连续化将其转化为带有相对大小关系的连续值；对于不存在"序"关系的属性，可根据属性取值将其拆解为多个属性 (one-hot 编码)

> [!NOTE] 闭式解⭐
> "闭式解"或称为"解析解"。闭式解是指可以通过具体的表达式解出待解参数，机器学习算法很少有闭式解，线性回归是一个特例 

### 多元线性回归
更一般的情形是样本由 $d$ 个属性描述，此时我们试图学得：
$$
f(x_i) = w^T x_i + b \quad \text{使得} \quad f(x_i) \simeq y_i
$$
对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：
将参数向量 $\hat{w}$ 表示为：
$$
\hat{w} = (\omega; b) = \begin{pmatrix} \omega_1 \\ \omega_2 \\ \vdots \\ \omega_d \\ b \end{pmatrix} \quad (d+1) \text{维}
$$
输入矩阵 $X$ 表示为：
$$
X = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1d} & 1 \\ x_{21} & x_{22} & \cdots & x_{2d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ x_{m1} & x_{m2} & \cdots & x_{md} & 1 \end{pmatrix} = \begin{pmatrix} a_1^T & 1 \\ a_2^T & 1 \\ \vdots & \vdots \\ a_m^T & 1 \end{pmatrix} \quad m \times (d+1)
$$
将输入矩阵 $X$ 与参数向量 $\hat{w}$ 相乘，得到预测值：
$$
X * \hat{w} = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1d} & 1 \\ x_{21} & x_{22} & \cdots & x_{2d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ x_{m1} & x_{m2} & \cdots & x_{md} & 1 \end{pmatrix} * \begin{pmatrix} \omega_1 \\ \omega_2 \\ \vdots \\ \omega_d \\ b \end{pmatrix} = \begin{pmatrix} \omega_1 x_{11} + \omega_2 x_{12} + \cdots + \omega_d x_{1d} + b \\ \omega_1 x_{21} + \omega_2 x_{22} + \cdots + \omega_d x_{2d} + b \\ \vdots \\ \omega_1 x_{m1} + \omega_2 x_{m2} + \cdots + \omega_d x_{md} + b \end{pmatrix} = \begin{pmatrix} f(x_1) \\ f(x_2) \\ \vdots \\ f(x_m) \end{pmatrix}
$$
最小二乘法的目标是最小化预测值与实际值之间的误差平方和：
$$
\hat{w}^* = \arg \min_{\hat{w}} \left( y - X\hat{w} \right)^T \left( y - X\hat{w} \right)
$$
> [!ABSTRACT] 解释📔
>  **$\arg \min_{\hat{w}}$*
>  - **含义**：表示“使得后面的表达式最小化的 $\hat{w}$ 值”。
>  - **作用**：这是一个数学符号，用于表示我们在所有可能的 $\hat{w}$ 值中寻找一个特定的值，这个值使得后面的误差函数最小。
>  
>  **$\left( y - X\hat{w} \right)^T$**
>  - **含义**：表示预测值与实际值之差的转置。
>  - **作用**：$y$ 是实际值向量，$X\hat{w}$ 是预测值向量。它们的差 $y - X\hat{w}$ 表示每个样本的预测误差。转置操作 $^T$ 是将向量或矩阵的行和列互换，为后续的矩阵乘法做准备。
>  
>  **$\left( y - X\hat{w} \right)$*
>  - **含义**：表示预测值与实际值之差。
>  - **作用**：与上面的转置部分相乘，计算误差的平方和。

同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0。
> [!NOTE] tip⭐
>  当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置\*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算 (比如[[03 pages/301机器学习/正则化\|正则化]])

对 $\hat{w}$ 求导，得到：
$$
\frac{\partial E_{\hat{w}}}{\partial \hat{w}} = 2X^T \left( X\hat{w} - y \right) = 0
$$
解上述方程，得到最优解的闭式解：
$$
\hat{w}^* = \left( X^TX \right)^{-1} X^T y
$$
其中，$X^TX$ 需要是满秩矩阵或非奇异矩阵，以确保逆矩阵存在。
使用最优解 $\hat{w}^*$ 进行预测：
$$
f(x_i) = \hat{x}_i^T \left( X^TX \right)^{-1} X^T y
$$



## **下面没搞完分割线**
补充多元函数基础知识：[[00 inbox/梯度\|梯度]]
[西瓜书第三章--线性模型 - Anrys_Tian - 博客园](https://www.cnblogs.com/icetree/p/12418775.html)
[《机器学习》（西瓜书）·周志华·第3章 线性模型-学习笔记 - 知乎](https://zhuanlan.zhihu.com/p/393610228)
[周志华《机器学习》“西瓜书”+“南瓜书”笔记：第3章 线性模型](https://www.zhihu.com/tardis/bd/art/499358024?source_id=1001)
### 对数线性回归
另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：

![7.png](https://i.loli.net/2018/10/17/5bc722b103cbf.png)

更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。

![8.png](https://i.loli.net/2018/10/17/5bc722b0a2841.png)

## **3.2 线性几率回归**

回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。

![9.png](https://i.loli.net/2018/10/17/5bc722b0c7748.png)

![10.png](https://i.loli.net/2018/10/17/5bc722b0a655d.png)

若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。

![11.png](https://i.loli.net/2018/10/17/5bc723b824f0c.png)

![12.png](https://i.loli.net/2018/10/17/5bc723b817961.png)

## **3.3 线性判别分析**

线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：

![13.png](https://i.loli.net/2018/10/17/5bc723b863ebb.png)![14.png](https://i.loli.net/2018/10/17/5bc723b85bfa9.png)

想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。

- 类内散度矩阵（within-class scatter matrix）
    

![15.png](https://i.loli.net/2018/10/17/5bc723b8156e1.png)

- 类间散度矩阵(between-class scaltter matrix)
    

![16.png](https://i.loli.net/2018/10/17/5bc723b7e9db3.png)

因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。

![17.png](https://i.loli.net/2018/10/17/5bc723b7e8a61.png)

从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。

![18.png](https://i.loli.net/2018/10/17/5bc723b83d5e0.png)

若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。 ​ 
## **3.4 多分类学习**

现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。

- OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。
    
- OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。
    
- MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。
    

![19.png](https://i.loli.net/2018/10/17/5bc723b862bfb.png)

![20.png](https://i.loli.net/2018/10/17/5bc723b8300d5.png)

## **3.5 类别不平衡问题**

类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：

1. 在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。
    
2. 在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。
    
3. 直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。![21.png](https://i.loli.net/2018/10/17/5bc726fe87ae2.png)
    

​