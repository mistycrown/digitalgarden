---
{"dg-publish":true,"permalink":"/02 resources/西瓜书1 绪论/","created":"2025-03-12T18:28:37.873+08:00","updated":"2025-03-12T22:10:04.002+08:00"}
---

## **1.1 [[03 pages/301机器学习/机器学习\|机器学习]]的定义**
机器学习：让计算机来学习经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断。
Mitchell 给出了一个形式化的定义，假设：
 - P：计算机程序在某任务类 T 上的性能。
 - T：计算机程序希望实现的任务类。
 - E：表示经验，即历史的数据集。
若该计算机程序通过利用经验 E 在任务 T 上获得了性能 P 的改善，则称该程序对 E 进行了学习。

> [!TIP] 算法和模型的区别💡
> "[[03 pages/302编程开发/算法\|算法]]"是指从数据中学得"[[03 pages/102思维认知/模型\|模型]]"的具体方法，例如后续章节中将会讲述的线性回归、对数几率回归、决策树等。"算法"产出的结果称为"模型"，通常是具体的函数或者可抽象地看作为函数，例如一元线性回归算法产出的模型即为形如 $f(x)=wx+b$ 的一元一次函数。 

## **1.2 机器学习的一些基本术语**
假设我们收集了一批西瓜的数据，例如：（色泽=青绿; 根蒂=蜷缩; 敲声=浊响)， (色泽=乌黑; 根蒂=稍蜷; 敲声=沉闷)， (色泽=浅自; 根蒂=硬挺; 敲声=清脆)……每对括号内是一个西瓜的记录，定义：	 
- 每个西瓜：一个==特征向量==（feature vector）。
	- 每一条记录为：一个实例（instance）或==样本==（sample）。
	- 例如：色泽或敲声，单个的特点为==特征==（feature）或属性（attribute）。
	- 对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，如果用色泽、根蒂和敲声这 3 个特征来刻画西瓜， 一个西瓜 $\boldsymbol{x}=(\text{青绿};\text{蜷缩};\text{清脆})$ **（向量中的元素用分号";"分隔时表示此向量为列向量，用逗号","分隔时表示为行向量）**。
	- 一个样本的特征数为：==维数==（dimensionality），该西瓜的例子维数为 3，当维数非常大时，也就是现在说的“维数灾难”。
	- ==样本空间==：也称为"输入空间"或"属性空间"。
		- 由于样本采用的是标明各个特征取值的"特征向量"来进行表示，根据线性代数的知识可知，有向量便会有向量所在的空间，因此称表示样本的特征向量所在的空间为样本空间，通常用花式大写的 $\mathcal{X}$ 表示。
- 所有记录的集合为：==数据集==。
	- 令集合 $D=\{\boldsymbol{x}_{1},\boldsymbol{x}_{2},...,\boldsymbol{x}_{m}\}$ 表示包含 $m$ 个样本的数据集，一般同一份数据集中的每个样本都含有相同个数的特征，假设此数据集中的每个样本都含有 $d$ 个特征，则第 $i$ 个样本的数学表示为 $d$ 维向量：$\boldsymbol{x}_{i}=(x_{i1};x_{i2};...;x_{id})$，其中 $x_{ij}$ 表示样本 $\boldsymbol{x}_{i}$ 在第 $j$ 个属性上的取值。
	- 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“==训练样本==”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“==测试样本==”。定义：	
		- 所有训练样本的集合为：==训练集==（trainning set），[特殊]。
		- 所有测试样本的集合为：==测试集==（test set），[一般]。  
	- 机器学习出来的模型适用于新样本的能力为：==泛化能力==（generalization），即从特殊到一般。
- ==标记==：机器学习的本质就是在学习样本在某个方面的表现是否存在潜在的规律，我们称该方面的信息为"标记"。一般第 $i$ 个样本的标记的数学表示为 $y_i$，标记所在的空间称为"==标记空间=="或"输出空间"，数学表示为花式大写的 $\mathcal{Y}$。标记通常也看作为样本的一部分，因此，一个完整的样本通常表示为 $(\boldsymbol{x}, y)$。
	- 标记的类型
		- 预测值为离散值的问题为：分类（classification）。
			- 二分类通常将正类记为 $1$，反类记为 $0$，即 $\mathcal{Y}=\{0,1\}$。
		- 预测值为连续值的问题为：回归（regression）。
			- 由于是连续型，因此标记的所有可能取值无法直接罗列，通常只有取值范围，回归任务的标记取值范围通常是整个实数域 $\mathbb{R}$，即 $\mathcal{Y}=\mathbb{R}$。
	- 有/无标记信息
		- 训练数据有标记信息的学习任务为：[[03 pages/301机器学习/监督学习\|监督学习]]（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。
		- 训练数据没有标记信息的学习任务为：[[无监督学习\|无监督学习]]（unsupervised learning），常见的有聚类和关联规则。
- 数据决定模型的上限，而算法则是让模型无限逼近上限
	- 数据决定模型的上限
		- 数据量：越大，模型效果越好
		- [[03 pages/301机器学习/特征工程\|特征工程]]：对特征数值化越合理，特征收集越全越细致，模型效果通常越好
	- 算法则是让模型无限逼近上限
		- 用各种可适用的算法从数据中学习其潜在的规律进而得到模型，不同的算法学习得到的模型效果自然有高低之分，效果越好则越逼近上限，即逼近真相。

## 1.3 假设空间
-  **假设空间（Hypothesis Space）**
	- **定义**：所有可能的从输入空间到输出空间的映射（假设）组成的集合。它是模型选择的范围，包含了所有可能的解决方案。
- **版本空间（Version Space）**
	- **定义**：假设空间中与训练数据一致的假设的子集。它是通过学习过程筛选出的有效假设集合。
		- **特点**：
		  - **一致性**：仅保留与训练样本（正例和反例）完全一致的假设。
		  - **构建过程**：通过删除与正例矛盾或与反例匹配的假设得到。例如：
		    1. 删除无法覆盖正例的假设（如规则无法正确分类好瓜）。
		    2. 删除覆盖反例的假设（如规则错误地将坏瓜判定为好瓜）。

## 1.4 归纳偏好
- 归纳偏好（inductive bias）的定义
	- 机器学习算法在学习过程中，对某种类型假设的**偏好**，称为归纳偏好。
	- **任何一个有效的机器学习算法必有其归纳偏好。** 若其不存在归纳偏好，则训练集中假设都“等效”，则在测试过程中，训练集的假设随机选择，得到的结果时好时坏，产生波动，则结果无意义。
- “奥卡姆剃刀”（Occam's razor）原则
	- 若有多个假设与观察一致，则选择最简单的那个（使模型结构尽量简单）。
	- 其实，并不能完全遵循“奥卡姆剃刀”原则，因为对于“模型哪个更简单？”的定义是模糊的，并没有一个确切的标准说哪个模型最简单，并且最简单的模型一定与问题密切相关。因此，最常用的方法则是基于模型在测试集上的表现来评判模型之间的优劣
- “没有免费的午餐定理”（No Free Lunch Theorem）
	- 无论学习算法A多聪明，学习算法B多笨拙，他们的期望性能相同。  
		- **重要前提：** 所有问题出现的机会相同、或所有问题同等重要。
	- 机器学习算法之间没有绝对的优劣之分，只有是否适合当前待解决的问题之分