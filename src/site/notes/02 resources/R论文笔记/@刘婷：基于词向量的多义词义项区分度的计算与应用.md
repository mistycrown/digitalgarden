---
{"dg-publish":true,"permalink":"/02 resources/R论文笔记/@刘婷：基于词向量的多义词义项区分度的计算与应用/","tags":["数字人文"],"created":"2025-08-22T10:38:57.963+08:00","updated":"2025-08-26T10:06:50.053+08:00"}
---


## 核心摘要

- **研究目标**: 利用上下文相关词向量，量化计算《现代汉语词典》中多义词的**义项区分度**。
- **核心方法**: 采用 **SimCSE-RoBERTa** 模型生成上下文相关词向量，通过 `1 - 余弦相似度` 的公式计算义项间的区分度。
- **主要发现**:
    - 176个双义词样本的义项区分度主要集中在 **$0.1 \sim 0.3$** 区间。
    - 区分度数值大小与词语的**句法特征**和**语义特征**相关。
- **应用价值**: 为辞书编纂的义项划分、词汇语义研究和语言教学提供量化参考。

## 1. 引言：问题与背景

- **研究背景**:
    - 辞书编纂对多义词的义项划分存在**主观性**和**标准不统一**的问题。
- **核心概念**: **义项区分度 (senses distinction)**
    - 定义: 衡量不同词义用法差异的量化指标。
    - 作用:
        - **区分度小**: 义项差异小，可考虑合并。
        - **区分度大**: 义项差异大，应分立。
- **主要方法回顾**:
    - **基于人工**: 精度高，但**耗时耗力**。
    - **基于义类体系**: 依赖于现有语义辞书，**扩展性差**。
    - **基于词向量**: **客观、全局性好**，能反映词语在海量语料中的用法差异。

## 2. 基于词向量的义项区分度计算方法

### 2.1 词向量技术

- **静态词向量 (Word2Vec, GloVe)**:
    - 一词一向量，无法区分多义词在不同上下文中的含义。
- **上下文相关词向量 (ELMo, BERT)**:
    - **一词多向量**，根据上下文动态生成词向量，适合处理一词多义问题。

### 2.2 义项区分度的计算

- **核心逻辑**:
    - 义项区分度与义项间语义相似度呈**负相关**。
- **计算公式**:
    - **语义相似度**: 使用两个义项向量 $\nu_i, \nu_j$ 的**余弦相似度**。
        $$ \cos ( \nu_i, \nu_j ) = \frac { \nu_i \cdot \nu_j } { | \nu_i | | \nu_j | } $$
    - **义项区分度**: 对余弦相似度进行线性变换。
        $$ f(s_i, s_j) = 1 - \cos(\nu_i, \nu_j) $$
    - **数值范围**: $0 \sim 1$。值越大，区分度越高。

## 3. 《现汉》多义词实验

### 3.1 数据与模型

- **研究样本**: 《现汉》第7版中 **176个** 双义词（名词、动词、形容词、兼类词）。
- **实验语料**: 8800个已进行人工义项标注的例句。
- **计算模型**: **SimCSE-RoBERTa** 模型。
    - 优势: 采用对比学习，改善了原生BERT向量空间稀疏问题，语义表征能力更强。
- **词向量生成**: 拼接模型**最后4个隐藏层**的输出向量，以捕获深层语义信息。

### 3.2 计算流程

1.  **例句标注**: 人工为每个词的50个例句标注义项。
2.  **生成词向量集**: 用模型计算每个例句中目标词的词向量。
3.  **生成义项中心向量**:
    - 对每个义项对应的所有词向量，先用 **PCA** 主成分分析过滤冗余信息。
    - 再对降维后的向量矩阵进行**平均池化 (average pooling)**，得到一个代表该义项语义中心的向量。
4.  **计算区分度**: 使用 $1 - \cos(\cdot)$ 公式计算两个义项中心向量的区分度。

### 3.3 实验结果

- **整体分布**: 绝大部分词的区分度分布在 $0.1 \sim 0.6$ 之间。
- **峰值区间**: **$0.2 \sim 0.3$** (占比 37.5%)。
- **平均值**: **0.3107**。
- 结论: 总体义项区分度偏低。

## 4. 结果分析：影响区分度的因素

- **核心观点**: 义项区分度的高低取决于**形式特征（句法、语义）** 的数量与显著程度。

### 4.1 句法特征 vs. 语义特征

- **关联性引申义** (如：自动-使动关系 "缓和")：
    - 更依赖**句法特征**区分（如：是否带宾语）。
- **相似性比喻义** (如：本义-比喻义 "桥梁")：
    - 更依赖**语义特征**区分（如：搭配对象的具体/抽象）。
- **对比结论**:
    - 比喻义的平均区分度 (0.3804) 略高于自动-使动义 (0.3310)，但差距不显著。
    - 两种特征在语境中**相互交织、补偿**。

### 4.2 不同区分度区间的词语特征

- **低区分度 (主要靠句法)**:
    - **特点**: 语义差别细微，主要通过句法功能（词性、句子成分）区分。
    - **例子**: “机密” (0.1833) - 形容词义项 vs. 名词义项。
- **中区分度 (主要靠语义)**:
    - **特点**: 句法特征相似，需依赖上下文的特定语义搭配来区分。
    - **例子**: “音响” (0.2375) - 指设备 vs. 指声音效果。
- **高区分度 (句法、语义兼备)**:
    - **特点**: 句法功能和语义搭配都具有**排他性**，特征互补，易于区分。
    - **例子**: “富有” (0.521) - 形容词义项（有钱） vs. 动词义项（具有）。

## 5. 义项区分度的应用价值

1.  **评估义项切分粒度**:
    - 区分度**过低**的词（如 "登山" 0.1262），可能意味着其在辞书中的义项划分**过细**。
2.  **建立词义可区分度等级**:
    - 为不同词、不同词类的多义词提供**统一的、可量化的比较标准**。
    - 指导词汇教学，重点关注区分度低的词语。
3.  **审视辞书体系的一致性**:
    - 结果显示《现汉》整体区分度偏低，说明义项划分**总体偏细**。
    - 可发现标准不一的问题，如 "上涨" (单义) vs "暴涨" (多义，区分度0.2432)，后者可考虑合并。

## 6. 结语

- **本文结论**:
    - 提出的计算方法能有效量化义项区分度。
    - 传统语文辞书的义项划分总体**偏细**，更侧重意义辨析而对形式特征考量不足，**系统一致性有待提升**。
- **未来研究**:
    - 扩大词汇样本范围。
    - 采用性能更强的语言模型。
    - 实现语料处理与计算的自动化。




> [!TIP] 如果说我想用SimCSE-RoBERTa这样的模型来获取一个词的静态词向量，应该怎么做？💡
> 当然可以。这是一个非常棒的实践性问题，因为它正好触及了如何“逆用”上下文相关模型来满足传统需求的场景。
> 
> **核心思想**：BERT/RoBERTa这类模型天生是**动态**的，它没有一个固定的“词典”来存储每个词的静态向量。为了模拟或创建一个静态词向量，我们必须通过**聚合（Aggregation）** 的方式，将一个词在**多种不同上下文**中的动态向量，融合成一个能代表其“平均”或“通用”含义的**单一向量**。
> 
> 以下是两种最主流和有效的方法，从“最佳实践”到“快速近似”排序：
> 
> ---
> 
> ### 方法一：多语境平均法 (Best Practice)
> 
> 这是最严谨、效果也最好的方法。它生成的向量最能代表一个词在真实语言环境中的普遍意义。
> 
> **原理**：一个词的通用含义，可以看作是它在所有可能句子中具体含义的平均值。
> 
> **操作步骤**：
> 
> 1.  **准备一个大型语料库**：
>     *   你需要一个包含大量不同主题和风格句子的文本库（比如：维基百科中文语料、新闻语料等）。语料库越大、越多样，结果越好。
> 
> 2.  **检索所有相关例句**：
>     *   从语料库中，找出所有包含你目标词（例如“银行”）的句子。
>     *   *“中国人民**银行**发布了新的利率政策。”*
>     *   *“他坐在河边的**银行**上休息。”*
>     *   *“我需要去**银行**办理汇款业务。”*
>     *   ... (检索出成百上千个这样的句子)
> 
> 3.  **逐句生成上下文向量**：
>     *   将每一个检索到的句子输入到 `SimCSE-RoBERTa` 模型中。
>     *   对于每个句子，提取出目标词“银行”对应的上下文相关词向量。
> 
> 4.  **聚合所有向量**：
>     *   现在你拥有了成百上千个代表“银行”在不同语境下含义的向量。
>     *   最简单的聚合方法就是**平均池化 (Average Pooling)**：将所有这些向量按位相加，然后除以向量的总数。
>     *   得到的最终结果就是一个单一的、高维度的向量。这个向量就近似为你想要的“银行”的静态词向量。
> 
> **优点**：
> *   **鲁棒性强**：因为它综合了词语的多种用法（包括主要含义和次要含义），能很好地捕捉词语的“语义中心”。
> *   **质量最高**：在计算词语间相似度等任务上，这种方法生成的静态向量表现通常最好。
> 
> **缺点**：
> *   **计算成本高**：需要处理大量句子，对计算资源有一定要求。
> 
> ---
> 
> ### 方法二：模板填充法 (Quick & Dirty)
> 
> 如果你没有大型语料库，或者需要一个快速的解决方案，这是一个不错的近似方法。
> 
> **原理**：将目标词放入一个非常中性、无偏见的“模板句”中，让模型在几乎没有上下文的情况下，输出一个接近其“基准”含义的向量。
> 
> **操作步骤**：
> 
> 1.  **设计一个或多个模板**：
>     *   模板应该尽可能通用，不提供任何具体的语义偏向。常见的模板是利用 `[MASK]` 标记。
>     *   例如：“**这是一个[MASK]。**”
> 
> 2.  **填充目标词并获取向量**：
>     *   将模板中的 `[MASK]` 替换为你的目标词，形成一个句子。
>     *   例如，输入句子：“**这是一个银行。**”
>     *   将这个句子喂给 `SimCSE-RoBERTa` 模型，并提取“银行”这个词的输出向量。
> 
> 3.  **（可选）使用多个模板求平均**：
>     *   为了结果更稳定，你可以设计多个不同的中性模板：
>         *   “关于**银行**这个词。”
>         *   “**银行**的定义是……”
>     *   分别获取向量后，再将它们平均，效果会比单一模板更好。
> 
> **优点**：
> *   **速度快，成本低**：只需要处理几个句子即可。
> *   **无需外部语料**：非常方便。
> 
> **缺点**：
> *   **可能存在偏差**：模板本身可能会引入微小的语义偏向。
> *   **无法体现词频分布**：无法像方法一那样，根据词义在真实世界中的使用频率来加权。
> 
> ---
> 
> ### **一个重要的技术细节：中文词语的处理**
> 
> `BERT/RoBERTa`处理中文时，通常是基于**字 (Character)** 的。也就是说，当你输入“银行”时，模型会先将其拆分为“银”和“行”两个字。
> 
> 所以，在上述两种方法的第3步（提取向量）中，你需要：
> 1.  分别获取“银”和“行”两个字的输出向量。
> 2.  将这两个向量进行**平均**，得到的结果才代表“银行”这个词的向量。
> 
> ### 总结对比
> 
> | 特性 | 方法一：多语境平均法 | 方法二：模板填充法 |
> | :--- | :--- | :--- |
> | **核心思想** | 统计真实语境下的平均含义 | 创造无偏见的语境获取基准含义 |
> | **质量** | **非常高**，鲁棒性强 | **中等**，可用作快速近似 |
> | **成本** | 计算量大，需要语料库 | 计算量小，无需额外数据 |
> | **适用场景** | 学术研究、构建高质量词向量库 | 快速原型验证、轻量级应用 |
> 
> 对于严肃的研究或应用，**强烈推荐使用方法一**。对于临时的、小规模的需求，方法二是一个非常便捷的选择。