---
{"dg-publish":true,"permalink":"/04 Projects/项目/西瓜书2 模型评估与选择/","created":"2025-03-12T18:29:57.520+08:00","updated":"2025-08-19T15:20:44.118+08:00"}
---


## **2.1  经验误差与过拟合**
1. 我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：==误差==（error）。定义：	
	1. 在训练集上的误差称为训练误差（training error）或**经验误差**（empirical error）。
	2. 在测试集上的误差称为**测试误差**（test error）。
	3. 学习器在所有新样本上的误差称为**泛化误差**（generalization error）。
	4. **错误率**：$E=\frac{a}{m}$，其中$m$为样本个数，$a$为分类错误样本个数。
	5. **精度**：精度=1-错误率。
2. 显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即<u>把一些训练样本的自身特点当做了普遍特征</u>；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：
	1. 学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：[[03 pages/301机器学习/过拟合\|过拟合]]（overfitting）。
	2.  学习能太差，训练样本的一般性质尚未学好，称为：**欠拟合**（underfitting）。
	3. 在过拟合问题中，训练误差十分小，但测试误差较大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。

## **2.2 评估方法**
泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。
因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。
我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。当选定好模型和调参完成后，我们需要使用初始的数据集 D 重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。
### 训练集和测试集的划分方法
#### **2.3.1 留出法**
将数据集 D 划分为两个互斥的集合，一个作为训练集 S，一个作为测试集 T，满足 D=S∪T 且 S∩T=∅，常见的划分为：大约 2/3-4/5 的样本用作训练，剩下的用作测试。
需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取==分层抽样==。
同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。
#### **2.3.2 交叉验证法**
将数据集 D 划分为 k 个大小相同的互斥子集，满足 D=D 1∪D 2∪...∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。
交叉验证法的思想是：每次**用 k-1 个子集的并集作为训练集，余下的那个子集作为测试集**，这样就有 K 种训练集/测试集划分的情况，从而可进行 k 次训练和测试，最终返回 k 次测试结果的均值。交叉验证法也称“k 折交叉验证”，k 最常用的取值是 10，下图给出了 10 折交叉验证的示意图。
![](https://i.loli.net/2018/10/17/5bc718115d224.png)
与留出法类似，将数据集 D 划分为 K 个子集的过程具有随机性，因此 K 折交叉验证通常也要重复 p 次，称为 ==p 次 k 折交叉验证==，常见的是 10 次 10 折交叉验证，即进行了 100 次训练/测试。
特殊地当划分的 k 个子集的每个子集中只有一个样本时，称为“==留一法==”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。
交叉验证法==本质上是在进行多次留出法==，且每次都换不同的子集做测试集，最终让所有样本均至少做1次测试样本。这样做的理由其实很简单，因为一般的留出法只会划分出1组训练集和测试集，仅依靠1组训练集和测试集去对比不同算法之间的效果显然不够置信，偶然性太强，因此要想基于固定的数据集产生多组不同的训练集和测试集，则只有进行多次划分，每次采用不同的子集作为测试集，也即为交叉验证法。
#### **2.3.3 自助法**
我们希望评估的是用整个 D 训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比 D 小，这必然会**引入一些因训练样本规模不同而导致的估计偏差**。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。
自助法的基本思想是：给定包含 m 个样本的数据集 D，每次随机从 D 中挑选一个样本，将其拷贝放入 D'，然后再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到。重复执行 m 次，就可以得到了包含 m 个样本的数据集 D'。可以得知在 m 次采样中，样本始终不被采到的概率取极限为：
$$ \lim _ { m \rightarrow \infty } ( 1 - \frac { 1 } { m } ) ^ { m } \rightarrow \frac { 1 } { e } \approx 0 . 3 6 8$$
这样，通过自助采样，初始样本集 D 中大约有 36.8%的样本没有出现在 D'中，于是可以**将 D'作为训练集，D-D'作为测试集**。
自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）==改变了初始数据集的分布==，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。
### **算法参数（超参数）与模型参数**
大多数学习算法都有些参数 (parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的"参数调节"或简称"调参" (parameter tuning)。
学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：**对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行**。
==算法参数==是指算法本身的一些参数（也称超参数），例如 $k$ 近邻的近邻个数 $k$、支持向量机的参数 $C$（详见"西瓜书"第6章式 (6.29)）。
算法配置好相应参数后进行训练，训练结束会得到一个模型，例如支持向量机最终会得到 $\boldsymbol{w}$ 和 $b$ 的具体数值（此处不考虑核函数），这就是==模型参数==。

### 验证集
带有参数的算法一般需要从候选参数配置方案中选择相对于当前数据集的最优参数配置方案，例如支持向量机的参数$C$，一般采用的是前面讲到的交叉验证法，但是交叉验证法操作起来较为复杂，实际中更多采用的是：<u>先用留出法将数据集划分出训练集和测试集，然后再对训练集采用留出法划分出训练集和新的测试集</u>，称新的测试集为**验证集**，接着基于验证集的测试结果来调参选出最优参数配置方案，最后将验证集合并进训练集（训练集数据量够的话也可不合并），用选出的最优参数配置在合并后的训练集上重新训练，再用测试集来评估训练得到的模型的性能。

## **2.3性能度量**
性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。
### 2.5.1 错误率与精度
在回归任务中，即预测连续值的问题，最常用的性能度量是“==均方误差==”（mean squared error）

![1.png](https://i.loli.net/2018/10/17/5bc71daf76276.png)

在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。

![2.png](https://i.loli.net/2018/10/17/5bc71daf4c704.png)

![3.png](https://i.loli.net/2018/10/17/5bc71daf6fb84.png)

### **2.5.2 查准率/查全率/F 1**
[[03 pages/301机器学习/二分类任务评估指标\|二分类任务评估指标]]
#### 查准率/查全率
错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：

![4.png](https://i.loli.net/2018/10/17/5bc71daf885a4.png)


#### P-R 曲线
正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。
“P-R 曲线”正是描述查准/查全率变化的曲线，P-R 曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的 P 值和 R 值，如下图所示：

![6.png](https://i.loli.net/2018/10/17/5bc71dafc4411.png)

若一个学习器 A 的 P-R 曲线被另一个学习器 B 的 P-R 曲线完全包住，则称：B 的性能优于 A。若 A 和 B 的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，<u>所以衍生出了“平衡点”（Break-Event Point，简称 BEP）</u>，即当 P=R 时的取值，平衡点的取值越高，性能更优。

#### F-Score 和 F 1
P 和 R 指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是 F-Measure，又称 [[ F-Score\| F-Score]]。F-Measure 是 P 和 R 的加权调和平均，$\beta>1$ 时查全率有更大影响；$\beta<1$ 时查准率有更大影响。即：

![7.png](https://i.loli.net/2018/10/17/5bc71daf40ff6.png)

![8.png](https://i.loli.net/2018/10/17/5bc71daf75407.png)

特别地，当β=1 时，也就是常见的 F 1 度量，是 P 和 R 的调和平均，当 F 1 较高时，模型的性能越好。

$$\begin{aligned} F1 &=\frac{2 \times P \times R}{P+R}=\frac{2 \times \frac{TP}{TP+FP} \times \frac{TP}{TP+FN}}{\frac{TP}{TP+FP}+\frac{TP}{TP+FN}} \ \\&=\frac{2 \times TP \times TP}{TP(TP+FN)+T P(TP+FP)} \ \\&=\frac{2 \times TP}{(TP+FN)+(TP+FP)}\ \\&=\frac{2 \times TP}{(TP+FN+FP+TN)+TP-TN}\ \\&=\frac{2 \times TP}{\text{样例总数}+TP-TN}\ \end{aligned}$$

#### 宏/微+P/R/F 1
[[03 pages/301机器学习/多分类任务评价指标\|多分类任务评价指标]]
有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练/执行多分类任务。
宏观就是先算出每个混淆矩阵的 P 值和 R 值，然后取得平均 P 值 macro-P 和平均 R 值 macro-R，再算出 Fβ或 F 1
微观是计算出混淆矩阵的平均 TP、FP、TN、FN，接着进行计算 P、R，进而求出 Fβ或 F 1。

![11.png](https://i.loli.net/2018/10/17/5bc71ed70230e.png)

"宏"没有考虑每个类别下的的样本数量，所以平等看待每个类别，因此会受到高 $P$ 和高 $R$ 类别的影响，而"微"则考虑到了每个类别的样本数量，因为样本数量多的类相应的 $TP$、$FP$、$TN$、$FN$ 也会占比更多，所以在各类别样本数量极度不平衡的情况下，数量较多的类别会主导最终结果。

### **2.5.3 ROC 与 AUC**
[[03 pages/301机器学习/ROC-AUC\|ROC-AUC]]
如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。
ROC 曲线正是从这个角度出发来研究学习器的泛化性能，ROC 曲线与 P-R 曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是 ROC 曲线以“==真正例率==”（True Positive Rate，简称 TPR）为横轴，纵轴为“==假正例率==”（False Positive Rate，简称 FPR），ROC 偏重研究基于测试样本评估值的排序好坏。

![12.png](https://i.loli.net/2018/10/17/5bc71ed6bee91.png)

![13.png](https://i.loli.net/2018/10/17/5bc71ed75cefe.png)

现实中的任务通常都是有限个测试样本，因此只能绘制出近似 ROC 曲线。
> [!ABSTRACT] 绘制方法📔
>  
> 假设我们已经训练得到一个学习器$f(\boldsymbol{s})$，现在用该学习器来对8个测试样本（4个正例，4个反例，即$m^+=m^-=4$）进行预测，预测结果为 **（此处用$\boldsymbol{s}$表示样本，以和坐标$(x,y)$作出区分）**：
> 
> $$\begin{aligned} &(\boldsymbol{s}_1,0.77,+),(\boldsymbol{s}_2,0.62,-),(\boldsymbol{s}_3,0.58,+),(\boldsymbol{s}_4,0.47,+),\ &(\boldsymbol{s}_5,0.47,-),(\boldsymbol{s}_6,0.33,-),(\boldsymbol{s}_7,0.23,+),(\boldsymbol{s}_8,0.15,-) \end{aligned}$$
> 
> 其中，$+$和$-$分别表示样本为正例和为反例，数字表示学习器$f$预测该样本为正例的概率，例如对于反例$\boldsymbol{s}_2$来说，当前学习器$f(\boldsymbol{s})$预测它是正例的概率为$0.62$。
> 绘制方法：
> - 首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。
> - 接着将分类阈值设为一个不可能取到的超大值，例如设为1。显然，此时所有样本预测为正例的概率都一定小于分类阈值，那么预测为正例的样本个数为0，相应的真正例率和假正例率也都为0，所以我们可以在坐标$(0,0)$处标记一个点。
> - 把分类阈值从大到小依次设为每个 样本的预测值，也就是依次设为0.77, 0.62, 0.58, 0.47, 0.33, 0.23,0.15，然后分别计算真正例率和假正例率，再在相应的坐标上标记点，最后再将各个点用直线连接, 即可得到$\text{ROC}$曲线。（在统计预测结果时，预测值等于分类阈值的样本也被算作预测为正例。）
> - 当分类阈值为 $0.77$ 时，测试样本 $\boldsymbol{s}_{1}$ 被预测为正例，由于它的真实标记也是正例，所以此时 $\boldsymbol{s}_{1}$ 是一个真正例。为了便于绘图，我们将 $x$ 轴（假正例率轴）的"步长"定为 $\frac{1}{m^-}$，$y$ 轴（真正例率轴）的"步长"定为 $\frac{1}{m^+}$。根据真正例率和假正例率的定义可知，每次变动分类阈值时，若新增 $i$ 个假正例，那么相应的 $x$ 轴坐标也就增加 $\frac{i}{m^-}$；若新增 $j$ 个真正例，那么相应的 $y$ 轴坐标也就增加 $\frac{j}{m^+}$。按照以上讲述的绘制流程，最终我们可以绘制出如图2-1所示的 $\text{ROC}$ 曲线。
> 
> ![图2-1 ROC曲线示意](https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/06419b28-ca3b-4719-a6df-60f81dc9d44b-roc.svg)
> 

同样地，进行模型的性能比较时，若一个学习器 A 的 ROC 曲线被另一个学习器 B 的 ROC 曲线完全包住，则称 B 的性能优于 A。若 A 和 B 的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。**ROC 曲线下的面积定义为 AUC（Area Uder ROC Curve）**。越好的学习器，其$\text{ROC}$曲线上的点越靠左上角，相应的$\text{ROC}$曲线下的面积也越大，即AUC也越大。

![15.png](https://i.loli.net/2018/10/17/5bc71ed6e2c57.png)

### **2.5.4 代价敏感错误率与代价曲线**
#### 代价矩阵与代价敏感错误率
上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病-->有疾病只是增多了检查，但有疾病-->无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。

![16.png](https://i.loli.net/2018/10/17/5bc71ed6ed582.png)

在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率为：

![17.png](https://i.loli.net/2018/10/17/5bc71ed70bebe.png)
$$
\begin{aligned} E(f;D;cost) &=\frac{1}{m}\left(m^{+} \times \frac{1}{m^{+}} \sum_{\boldsymbol{x}_{i} \in D^{+}} \mathbb{I}\left(f\left(\boldsymbol{x}_{i} \neq y_{i}\right)\right) \times cost_{+-}+m^{-} \times \frac{1}{m^{-}} \sum_{\boldsymbol{x}_{i} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}_{i} \neq y_{i}\right)\right) \times cost_{-+}\right) \ \\&=\frac{m^{+}}{m} \times \frac{1}{m^{+}} \sum_{\boldsymbol{x}_{i} \in D^{+}} \mathbb{I}\left(f\left(\boldsymbol{x}_{i} \neq y_{i}\right)\right) \times cost_{+-}+\frac{m^{-}}{m} \times \frac{1}{m^{-}} \sum_{\boldsymbol{x}_{i} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}_{i} \neq y_{i}\right)\right) \times cost_{-+} \end{aligned}
$$
其中$m^{+}$和$m^{-}$分别表示正例集$D^{+}$和反例集$D^{-}$的样本个数。
$\frac{1}{m^{+}} \sum_{\boldsymbol{x}_{i} \in D^{+}} \mathbb{I}\left(f\left(\boldsymbol{x}_{i} \neq y_{i}\right)\right)$表示正例集$D^{+}$中预测错误样本所占比例，即假反例率FNR。
$\frac{1}{m^{-}} \sum_{\boldsymbol{x}_{i} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}_{i} \neq y_{i}\right)\right)$表示反例集$D^{-}$中预测错误样本所占比例，即假正例率FPR。
$\frac{m^{+}}{m}$表示样例集$D$中正例所占比例，或理解为随机从$D$中取一个样例取到正例的概率。
$\frac{m^{-}}{m}$表示样例集$D$中反例所占比例，或理解为随机从$D$中取一个样例取到反例的概率。
因此，若将样例为正例的概率$\frac{m^{+}}{m}$记为$p$，则样例为f反例的概率$\frac{m^{-}}{m}$为$1-p$，上式可进一步写为
$$E(f;D;cost)=p \times \mathrm{FNR} \times cost_{+-}+(1-p) \times \mathrm{FPR} \times cost_{-+}$$

#### 代价曲线
同样对于 ROC 曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中 p 表示正例的概率，纵轴是取值为[0,1]的归一化代价。

![18.png](https://i.loli.net/2018/10/17/5bc71ed6e952e.png)

![19.png](https://i.loli.net/2018/10/17/5bc71ed6eee7b.png)

代价曲线的绘制：设 ROC 曲线上一点的坐标为 (TPR，FPR) ，则可相应计算出 FNR，然后在代价平面上绘制一条从 (0，FPR) 到 (1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将 ROC 曲线上的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：

![20.png](https://i.loli.net/2018/10/17/5bc71ed716e0d.png)

## 2.4 比较检验
我们希望比较的是 **泛化性能**，但是我们通过实验评估方法获得的是测试集上的性能，两者未必相同；测试集上的性能与测试集的选择有很大的关系，且对于不同大小的测试集也会得到不同的结果；机器学习算法本身具有一定的随机性：即用相同的参数设置在同一个测试集上运行多次，产生的结果可能也不一样。
**统计假设检验**(hypothesis test)为我们进行学习器性能比较提供了依据。基于假设检验结果我们可以推断出：若在测试集上观察到学习器$A$比$B$好，则$A$的泛化性能是否**在统计意义上优于**$B$，以及这个结论的把握有多大。下面介绍两种最基本的假设检验，然后介绍几种常用的机器学习性能比较的方法。为方便讨论，本大节默认以错误率为性能度量，用 $\epsilon$ 表示.
### 2.4.1 假设检验
**假设检验** 中的“假设”是对学习器泛化错误率分布的某种判断或猜想，例如 $\epsilon = \epsilon_0$，我们在现实中并不知道学习器的泛化错误率，只能获知其测试错误率 $\hat{\epsilon}$，泛化错误率与测试错误率未必相同，但是直观上，二者是相近的，因此，我们可根据测试错误率估推出泛化错误率的分布。
泛化错误率为 $\epsilon$ 的学习器在一个样本上犯错误的概率是 $\epsilon$；测试错误率 $\hat{\epsilon}$ 意味着在 $m$ 个测试样本中恰有 $\hat{\epsilon} \times m$ 个被误分类，假定测试样本是从样本总体中独立采样得到，那么泛化错误率为 $\epsilon$ 的学习器将其中 $m'$ 个样本误分类、其余样本均分类正确的概率为：$(^{m}_{m'})\epsilon^{m'}(1-\epsilon)^{m-m'}$，由此可估算出恰将 $\hat{\epsilon} \times m$ 个样本误分类的概率如下式所示，这也表达了在包含 $m$ 个样本的测试集上，泛化错误率为 $\epsilon$ 的学习器被测得测试错误率为 $\hat{\epsilon}$ 的概率为：
$$P(\hat{\epsilon};\epsilon)=(^{m}_{\hat{\epsilon} \times m})\epsilon^{\hat{\epsilon}\times m}(1-\epsilon)^{m-\hat{\epsilon}\times m}$$
给定测试错误率，则解 $\partial P(\hat{\epsilon};\epsilon) / \partial \epsilon = 0$ 可得，$P(\hat{\epsilon};\epsilon)$ 在 $\epsilon = \hat{\epsilon}$ 时最大，$|\epsilon - \hat{\epsilon}|$ 增大的时候，$P(\hat{\epsilon};\epsilon)$ 减少，符合二项(binomial)分布，如下图所示，当 $\epsilon = 0.3$，则 10 个样本中测得 3 个被误分类的概率最大。
![Pasted image 20250312213622.png](/img/user/09%20settings/Z%20attachment/Pasted%20image%2020250312213622.png)
我们可以使用**二项检验**(binomial test)对 $\epsilon \le 0.3$ 进行检验，更一般的，我们可以有 $\epsilon \le \epsilon_0$，即在 $1 - \alpha$ 的概率内能观测到的最大错误率如下式进行计算，这里的 $1 - \alpha$ 反映了结论的**置信度**(confidence)，即相当于上图中非阴影部分的范围。

$$\overline{\epsilon} = min \epsilon s.t. \sum_{i=\epsilon \times m+1}^{m} \binom{m}{i} \epsilon_0^i (1 - \epsilon_0)^{m-i} < \alpha$$

s.t. 是 subject to 的简写，即使左边式子在右边条件满足时成立。
若此时测试错误率 $\hat{\epsilon}$ 大于临界值 $\overline{\epsilon}$，则有结论：在 $\alpha$ 的显著度下，假设 $\epsilon \le \epsilon_0$ **不能被拒绝**，则能以 $1 - \alpha$ 的置信度认为，学习器的泛化错误率不大于 $\epsilon_0$；否则假设可被拒绝，即在 $\alpha$ 的显著度下可认为学习器的泛化错误率大于 $\epsilon_0$

在很多时候我们并非只做一次留出法进行估计，而是多次重复留出法或是交叉验证法等进行多次训练/测试，这样就可以得到多个测试错误率，此时可以使用 ==t检验==(t-test)，假定我们得到了 $k$ 个测试错误率，$\hat{\epsilon_1}$，$\hat{\epsilon_2}$，$\cdot \cdot \cdot$，$\hat{\epsilon_k}$，则我们可以得到错误率 $\mu$，和方差 $\sigma^2$
$$\mu = \frac{1}{k} \sum_{i=1}^{k} \hat{\epsilon}$$
$$\sigma^2 = \frac{1}{k-1} \sum_{i=1}^{k} (\hat{\epsilon} - \mu)^2$$
考虑到这 $k$ 个测试错误率可看作泛化错误率 $\epsilon_0$ 的独立采样，则：
$$\tau_t = \frac{\sqrt{k} (\mu - \epsilon_0)}{\sigma}$$
服从自由度为 $k-1$ 的 t 分布，如下图所示：
![Pasted image 20250312215629.png](/img/user/09%20settings/Z%20attachment/Pasted%20image%2020250312215629.png)

对假设 $\mu = \epsilon_0$ 和显著度 $\alpha$，我们可以计算出当测试错误率均值为 $\epsilon_0$ 时，在 $1 - \alpha$ 的概率内能观测到的最大错误率，即临界值，这里考虑双边(two-tailed)假设，如上图所示，两边阴影各有 $\alpha / 2$ 的面积；假定阴影部分范围分别为 $(-\infty, t_{-\alpha/2}]$ 和 $[t_{\alpha/2}, \infty)$，若 $\tau_t$ 位于临界值范围 $[t_{-\alpha/2}, t_{\alpha/2}]$ 内，则不能拒绝假设 $\mu = \epsilon_0$，即可认为泛化错误率为 $\epsilon_0$，置信度为 $1 - \alpha$；否则可拒绝该假设，$\alpha$ 常用取值为0.05和0.1，下表给出一些常用临界值：
![Pasted image 20250312215854.png|441](/img/user/09%20settings/Z%20attachment/Pasted%20image%2020250312215854.png)

### 2.4.2 交叉验证t检验
后面的再说吧，看不懂一点 #待补充
[机器学习【西瓜书/南瓜书】--- 第2章模型评估与选择（下）（学习笔记+公式推导）_机器学习第二章假设检验公式-CSDN博客](https://chen-ac.blog.csdn.net/article/details/122778072?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-122778072-blog-126584769.235%5Ev43%5Econtrol&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-122778072-blog-126584769.235%5Ev43%5Econtrol&utm_relevant_index=5) 
[4.第2章 - 人工智能-周志华ML教材版-2-4_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1LrsoezEL8?spm_id_from=333.788.videopod.episodes&vd_source=26da385b1c7abd1742d76bdc415d9b93&p=5)


### 2.4.3 McNemar检验


### 2.4.4 Friedman检验与Nemenyi后续检验


## 2.5 偏差与方差

