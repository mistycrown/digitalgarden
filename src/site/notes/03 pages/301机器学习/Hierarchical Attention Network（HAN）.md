---
{"dg-publish":true,"permalink":"/03 pages/301机器学习/Hierarchical Attention Network（HAN）/","created":"2025-03-04T14:16:24.292+08:00","updated":"2025-03-04T14:21:51.339+08:00"}
---


Hierarchical Attention Network（HAN，层级注意力网络）是一种**面向长文本（如文档、篇章）分类的深度学习模型**，由 Zichao Yang 等人在 2016 年提出。其核心思想是通过模仿人类阅读文档时的层次化注意力机制，捕捉不同层次的文本重要性差异。以下是其核心解析：

### **1. 模型设计动机**
- **层次结构模拟**：文档由句子组成，句子由词语组成，HAN 通过层级结构（文档→句子→词）建模这一关系。
- **重要性差异**：文档中不同句子、同一句子中不同词语对分类的贡献不同，需动态分配注意力权重。

### **2. 模型架构**
HAN 分为四个模块，形成“**双向编码器+注意力机制**”的层级结构：

#### **(1) 词级别处理**
1. **词编码器（Word Encoder）**：  
   - 使用双向 GRU（或 [[03 pages/301机器学习/LSTM\|LSTM]]）对句子中的每个词进行编码，生成上下文感知的词表示。  
   - 输出：每个词的隐藏状态（正向+反向拼接）。

2. **词注意力（Word Attention）**：  
   - 通过可学习的上下文向量（Context Vector）计算各词的重要性权重。  
   - 权重越大，表示该词对当前句子的语义贡献越高。  
   - 输出：句子的向量表示（加权求和后的词向量）。

#### **(2) 句子级别处理**
1. **句子编码器（Sentence Encoder）**：  
   - 使用双向 GRU 编码所有句子的向量表示，捕捉句子间的上下文依赖。

2. **句子注意力（Sentence Attention）**：  
   - 类似词注意力，计算各句子的重要性权重。  
   - 输出：文档的最终向量表示（加权求和后的句子向量）。

#### **(3) 分类层**
- 将文档向量输入全连接层和 Softmax，输出类别概率。

---

### **3. 注意力机制详解**
- **核心公式**：  
  - 对于输入序列 $H = [h_1, h_2, ..., h_n]$，注意力权重计算：  
    $$
    U_i = \tanh (W h_i + b) \\
    \alpha_i = \frac{\exp (u_i^\top u)}{\sum_j \exp (u_j^\top u)}
$$
  - $W$ 和 $u$ 为可学习参数，$\alpha_i$ 表示第 $i$ 个元素的权重。

- **双向编码优势**：  
  双向 GRU 同时捕捉前后文信息，增强对长距离依赖的建模能力。

---

### **4. 技术特点与优势**
- **层次化建模**：  
  适合处理长文本（如评论、新闻），解决传统模型（TextCNN、RNN）对长文本建模不足的问题。
- **可解释性**：  
  注意力权重可视化可解释模型关注的关键词/句子（例如：分类为“差评”时，模型可能关注“延迟高”和“客服差”）。
- **灵活扩展**：  
  可替换编码器（如用 BERT 代替 GRU）或注意力机制（如 Self-Attention）。

### **5. 应用场景**
- **文档分类**：新闻主题分类、商品评论情感分析（如区分 Yelp 评论的 1-5 星）。  
- **长文本理解**：医学报告摘要生成、法律文书关键信息提取。  
- **多标签分类**：知乎问答的多标签分类（如同时标记“科技”“互联网”）。



