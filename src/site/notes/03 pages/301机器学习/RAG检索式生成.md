---
{"dg-publish":true,"permalink":"/03 pages/301机器学习/RAG检索式生成/","created":"2025-01-06T17:08:53.825+08:00","updated":"2025-03-08T14:05:07.226+08:00"}
---




#### **LLM 面临的主要问题**
1. **信息偏差/幻觉**：
    1. LLM 有时会生成与客观事实不符的信息，导致用户接收到不准确的信息。
        - **解决方案**：RAG 通过检索数据源辅助模型生成过程，确保输出内容的精确性和可信度，减少信息偏差。
2. **知识更新滞后性**：
    1. LLM 基于静态数据集训练，可能导致知识更新滞后，无法及时反映最新信息动态。
        - **解决方案**：RAG 通过实时检索最新数据，保持内容的时效性，确保信息的持续更新和准确性。
3. **内容不可追溯**：
    1. LLM 生成的内容往往缺乏明确的信息来源，影响内容的可信度。
        - **解决方案**：RAG 将生成内容与检索到的原始资料建立链接，增强内容的可追溯性，提升用户对生成内容的信任度。
4. **领域专业知识能力欠缺**：
    1. LLM 在处理特定领域的专业知识时，效果可能不理想，影响回答质量。
        - **解决方案**：RAG 通过检索特定领域的相关文档，为模型提供丰富的上下文信息，提升专业领域内的问题回答质量和深度。
5. **推理能力限制**：
    1. 面对复杂问题时，LLM 可能缺乏必要的推理能力，影响问题理解和回答。
        - **解决方案**：RAG 结合检索到的信息和模型的生成能力，通过提供额外的背景知识和数据支持，增强模型的推理和理解能力。
6. **应用场景适应性受限**：
    1. LLM 需在多样化应用场景中保持高效和准确，但单一模型可能难以全面适应所有场景。
        - **解决方案**：RAG 使得 LLM 能够通过检索对应应用场景数据，灵活适应问答系统、推荐系统等多种应用场景。
7. **长文本处理能力较弱**：
    1. LLM 在理解和生成长篇内容时受限于有限的上下文窗口，处理速度随着输入长度增加而减慢。
        - **解决方案**：RAG 通过检索和整合长文本信息，强化模型对长上下文的理解和生成，有效突破输入长度限制，降低调用成本，提升整体处理效率。
#### **RAG 的工作流程**
RAG 是一个完整的系统，其工作流程可以简单地分为以下四个阶段：
1. **数据处理**：
	1. - 收集和预处理相关数据，以确保信息的质量和可用性。
2. **检索阶段**：
	1. - 从知识库中检索与用户查询相关的信息，确保获取最新和最相关的数据。
3. **增强阶段**：
	1. - 将检索到的信息与用户输入结合，为模型提供丰富的上下文。
4. **生成阶段**：
	1. - 基于增强的信息，使用大型语言模型生成最终的回答或内容。


